#!/usr/bin/env python3
from __future__ import annotations
import sys
from pathlib import Path

# Path setup
here = Path(__file__).parent
for p in [here] + list(here.parents):
    if (p / "src").is_dir():
        sys.path.insert(0, str(p / "src"))
        break
else:
    raise RuntimeError("Could not locate 'src' folder to add to PYTHONPATH")

from typing import Tuple, Optional, Callable
from torch.utils.data import DataLoader, Dataset
from torchvision import datasets, transforms
import os
from random import randint, random
import h5py
import numpy as np
from PIL import Image, ImageDraw
import torch

# Find project root and set data directory
def find_project_root() -> Path:
    """Find the project root (directory containing 'src' folder)."""
    current = Path(__file__).parent
    for p in [current] + list(current.parents):
        if (p / "src").is_dir():
            return p
    # Fallback to current working directory
    return Path.cwd()

# <repo-root>/data by default
project_root = find_project_root()
default_data_dir = project_root / "data"

# Image counts used by the original generator
NTRAIN, NTEST = 9 * 10**4, 10**4

batch_size = 512

class MSDS1(Dataset):
    """
    Minimal PyTorch-style wrapper around the dataset generated by your script.

    • Each sample returns:
        img   –   torch.Tensor   (C, H, W)   –   by default in [0,1]
        label –   torch.Tensor   (4, 4, 6)   –   (x, y, angle, R, G, B)
    """

    def __init__(
        self,
        root: str | Path = "./msds1",
        split: str = "train",            # "train" | "test"
        transform: Optional[Callable] = None,
        target_transform: Optional[Callable] = None,
        preload_labels: bool = True,     # load the entire HDF5 once; fine for 90k × 96 ints ≈ 35 MB
    ):
        super().__init__()

        split = split.lower()
        if split not in {"train", "test"}:
            raise ValueError(f"split must be 'train' or 'test', got {split!r}")

        self.img_dir = Path(root) / split / "0"
        self.img_paths = sorted(self.img_dir.glob("*.png"))          # 00000.png, 00001.png, …
        if not self.img_paths:
            raise FileNotFoundError(f"No images found in {self.img_dir}")

        # ------ labels ------------------------------------------------------
        h5_path = Path(root) / f"{split}_labels.hdf5"
        if not h5_path.exists():
            raise FileNotFoundError(h5_path)

        if preload_labels:
            with h5py.File(h5_path, "r") as f:
                self.labels = torch.from_numpy(f["labels"][...])     # (N, 4, 4, 6)
        else:
            # keep only the path; open lazily in __getitem__
            self.labels = None
            self._h5_path = h5_path

        # ------ optional transforms ----------------------------------------
        from torchvision.transforms import ToTensor
        self.transform = transform or ToTensor()
        self.target_transform = target_transform

    def __len__(self) -> int:
        return len(self.img_paths)

    # --------------------------------------------------------------------- #
    def _lazy_load_label(self, idx: int) -> torch.Tensor:
        """Open the HDF5 file every time (safer with num_workers>0)."""
        with h5py.File(self._h5_path, "r") as f:
            arr = f["labels"][idx]                   # (4, 4, 6) ndarray
        return torch.from_numpy(arr)

    # --------------------------------------------------------------------- #
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        # --- image ---------------------------------------------------------
        img = Image.open(self.img_paths[idx]).convert("RGB")         # (32×32)
        if self.transform:
            img = self.transform(img)                                # torch.FloatTensor

        # --- label ---------------------------------------------------------
        if self.labels is not None:                                  # pre-loaded
            target = self.labels[idx]
        else:                                                        # lazy path
            target = self._lazy_load_label(idx)

        if self.target_transform:
            target = self.target_transform(target)

        return img, target

# ───────────────────────── dataset availability ──────────────────────────

def get_root(data_dir: Path) -> Path:
    return data_dir / "msds1"

def _exists(root: Path) -> bool:
    """Rough-and-ready: look for both splits, at least one PNG, and the HDF5s."""
    return (
        (root / "train/0").is_dir()
        and any((root / "train/0").glob("*.png"))
        and (root / "test/0").is_dir()
        and any((root / "test/0").glob("*.png"))
        and (root / "train_labels.hdf5").is_file()
        and (root / "test_labels.hdf5").is_file()
    )

def generate(split: str, n_samples: int, base_dir: Path) -> None:
    """
    Generate synthetic MSDS1 dataset with ellipse-based images.
    Each image is 32x32 with a 4x4 grid of colored ellipses.
    """
    print(f"Generating {n_samples} {split} samples in {base_dir.absolute()}")
    
    # Create directories - torchvision.datasets.ImageFolder requires a subfolder
    split_dir = base_dir / split / "0"
    split_dir.mkdir(parents=True, exist_ok=True)
    print(f"Created directory: {split_dir.absolute()}")
    
    labels = np.empty((n_samples, 4, 4, 6), dtype=np.int32)
    for count in range(n_samples):
        # Generate base color for all items in this image
        item_color = (
            randint(0, 191) + 32,
            randint(0, 191) + 32,
            randint(0, 191) + 32,
        )

        # Create 32x32 black background image
        img = Image.new('RGB', (32, 32), (0, 0, 0))
        
        # Generate 4x4 grid of ellipses
        for i in range(4):
            for j in range(4):
                # Create 8x8 transparent item
                item = Image.new('RGBA', (8, 8), (255, 255, 255, 0))
                draw = ImageDraw.Draw(item)

                # Color variation from base color
                now_color = (
                    item_color[0] + randint(-32, 32),
                    item_color[1] + randint(-32, 32),
                    item_color[2] + randint(-32, 32),
                    255,
                )
                
                # Draw ellipse (small horizontal ellipse)
                draw.ellipse((1, 3, 7, 5), now_color)
                
                # Random rotation
                now_angle = random() * 180
                item = item.rotate(now_angle, resample=Image.Resampling.BICUBIC)

                # Position with small random offset
                now_pos = (
                    i * 8 + randint(-1, 1),
                    j * 8 + randint(-1, 1),
                )
                
                # Paste item onto main image
                img.paste(item, now_pos, mask=item)

                # Store label: position (x, y), angle, and color (R, G, B)
                label = now_pos + (now_angle, ) + now_color[:3]
                labels[count, i, j, :] = label

        # Save image with zero-padded filename
        img_path = split_dir / f"{count:05d}.png"
        img.save(img_path, compress_level=1)
        
        if count % 10000 == 0 and count > 0:
            print(f"Generated {count}/{n_samples} {split} images")
    
    # Save labels to HDF5 with compression
    labels_path = base_dir / f"{split}_labels.hdf5"
    with h5py.File(labels_path, "w") as f:
        f.create_dataset('labels',
                        data=labels,
                        compression='gzip',
                        shuffle=True)
    
    print(f"Finished generating {n_samples} {split} images at {split_dir.absolute()}")
    print(f"Labels saved to {labels_path.absolute()}")

def _generate(root: Path) -> None:
    """Run the local generator to create msds1 under root."""
    try:
        root.mkdir(parents=True, exist_ok=True)
        # Call the generate function directly (no self-import needed)
        generate("train", NTRAIN, base_dir=root)
        generate("test", NTEST, base_dir=root)
    except Exception as e:
        raise RuntimeError(
            f"Failed to generate msds1 dataset: {e}"
        ) from e

def _ensure(root: Path) -> None:
    """Guarantees the dataset exists or raises RuntimeError."""
    if _exists(root):
        print(f"MSDS1 dataset found at {root}")
        return
    print(f"MSDS1 dataset not found at {root}")
    print(f"Generating MSDS1 dataset at: {root.absolute()}")
    _generate(root)
    if not _exists(root):
        raise RuntimeError(f"Generating msds1 failed – dataset still missing at {root.absolute()}")

# ───────────────────────── public loader function ────────────────────────

def get_msds1_dataloaders(
    data_dir: Path | str = default_data_dir,
    batch_size: int = 64,
    num_workers: int = 4,
) -> Tuple[DataLoader, DataLoader, DataLoader]:
    """
    • If data already present → just load it.
    • Else → call the generator once, then load.
    • Any generator failure is propagated.
    """
    data_dir = Path(data_dir)
    root = get_root(data_dir)
    
    print(f"Looking for MSDS1 dataset at: {root.absolute()}")
    print(f"Data directory: {data_dir.absolute()}")
    
    _ensure(root)
    
    train_tf = transforms.Compose(
        [transforms.RandomHorizontalFlip(), transforms.ToTensor()]
    )
    test_tf = transforms.ToTensor()
    
    train_ds = datasets.ImageFolder(root / "train", transform=train_tf)
    val_ds = datasets.ImageFolder(root / "test", transform=test_tf)  # uses 'test' split
    test_ds = val_ds  # same underlying set
    
    print(f"Created datasets - Train: {len(train_ds)} samples, Test: {len(val_ds)} samples")
    
    kw = dict(batch_size=batch_size, num_workers=num_workers, pin_memory=True)
    train_loader = DataLoader(train_ds, shuffle=True, **kw)
    val_loader = DataLoader(val_ds, shuffle=False, **kw)
    test_loader = DataLoader(test_ds, shuffle=False, **kw)
    
    return train_loader, val_loader, test_loader

# For direct execution testing
if __name__ == "__main__":
    print("Testing MSDS1 dataset generation...")
    loaders = get_msds1_dataloaders(default_data_dir, batch_size=32, num_workers=2)
    print(f"Successfully created dataloaders: {len(loaders)} loaders")
    
    # Test loading a batch
    train_loader = loaders[0]
    batch = next(iter(train_loader))
    print(f"Batch shape: {batch[0].shape}, Labels shape: {batch[1].shape}")